{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example of webscraping, we'd like to make a small library of images downloaded from Bing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust the path to your repo\n",
    "path_to_repo = '/Users/ruben/repo/personal/ga/DAT-23-NYC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 'cats with hats'  # only use normal characters (azAZ10) and spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the webpage\n",
    "- mine the html code\n",
    "- find the relevant images\n",
    "- download the images to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Do-It-Yourself Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The package `requests` let you make HTTP requests like GET and POST.\n",
    "- To install the package, simply type in your terminal:\n",
    "\n",
    "    ```pip install requests```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.bing.com/images/search?q=' + query.replace(' ', '\\%20')\n",
    "req = requests.get(url)\n",
    "html = req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:Web=\"http://schemas.live.com/Web/\"><head><meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\" /><script type=\"text/javascript\">//<![CDATA[\n",
      "si_ST=new Date\n",
      "//]]></script><script type=\"text/javascript\"\n"
     ]
    }
   ],
   "source": [
    "print html[:300]  # print first characters from html page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Have a look at how the html looks, and try to parse the file yourself.\n",
    "- We noticed that the search results are always of the form `<img class=\"img_hid\" src2=\"https://tse4.mm.bing.net/th?id=JN.8aaYIEhuqMgwcBHAyaTh2A&amp;w=221&amp;h=172&amp;c=7&amp;rs=1&amp;qlt=90&amp;o=4&amp;pid=1.1\" style=\"width:221px;height:172px;\" width=\"221\" height=\"172\"/>`\n",
    "- So let's look for the pattern `<img class=\"img_hid\" src2=\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 links\n"
     ]
    }
   ],
   "source": [
    "q = '<img class=\"img_hid\" src2=\"'\n",
    "\n",
    "urls = []\n",
    "pos, end = 0, 0\n",
    "while html[end:].find(q) >= 0:\n",
    "    pos = pos + html[pos:].find(q) + len(q)  # go to position of image url\n",
    "    end = pos + html[pos:].find('\"')  # look for end of image url\n",
    "    urls.append(html[pos:end])\n",
    "print \"Found\", len(urls), \"links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder /Users/ruben/repo/personal/ga/DAT-23-NYC/data/images/cats_with_hats/\n"
     ]
    }
   ],
   "source": [
    "# create folder and subfolders for the image library\n",
    "import os\n",
    "path_to_imgs = path_to_repo + 'data/images/' + query.replace(' ', '_') + '/'\n",
    "try:\n",
    "    os.makedirs(path_to_imgs)  \n",
    "except OSError as e:\n",
    "    if e.errno != 17:  # if folder already exists, no problem (just overwrite)\n",
    "        raise e  # otherwise, raise e again\n",
    "print \"Creating folder\", path_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35 images!\n"
     ]
    }
   ],
   "source": [
    "for no, url in enumerate(urls):\n",
    "    req = requests.get(url)\n",
    "    with open(path_to_imgs + '%03d' % no + '.jpeg', 'wb') as f:\n",
    "        f.write(req.content)\n",
    "print \"Saved\", len(urls), \"images!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The package `BeautifulSoup` will help you navigating through the HTML code\n",
    "- to install the package, simply type `pip install beautifulsoup4` \n",
    "- Documentation is available on their <a href=\"http://www.crummy.com/software/BeautifulSoup/\">website</a>\n",
    "- Especially have a look at their <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Quick Start</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of parsing the `html` page yourself, you feed it into `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup.BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `soup` object saves all the html page's attributes as class attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>cats\\ with\\ hats - Bing Images</title>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'title'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cats\\\\ with\\\\ hats - Bing Images'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'head'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common task is extracting all the URLs found within a pageâ€™s `<a>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search?q=cats%5c+with%5c+hats&FORM=HDRSC1\n",
      "/images?FORM=HDRSC2\n",
      "/videos/search?q=cats%5c+with%5c+hats&FORM=HDRSC3\n",
      "/maps/default.aspx?q=cats%5c+with%5c+hats&mkt=en&FORM=HDRSC4\n",
      "/news/search?q=cats%5c+with%5c+hats&FORM=HDRSC6\n"
     ]
    }
   ],
   "source": [
    "# Let's limit to first 5\n",
    "for link in soup.findAll('a')[:5]:  \n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look for all pictures.\n",
    "- We already saw earlier that the picture we're interested in are the ones with `<img>` tags with `src2` attributes, rather than `src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 images\n"
     ]
    }
   ],
   "source": [
    "# Let's look for all pictures with src2 attributes\n",
    "urls = []\n",
    "for img in soup.findAll('img'):\n",
    "    if img.get('src2'):\n",
    "        urls.append(img.get('src2'))\n",
    "print \"Found\", len(urls), \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder /Users/ruben/repo/personal/ga/DAT-23-NYC/data/images/cats_with_hats_bs/\n"
     ]
    }
   ],
   "source": [
    "# create folder and subfolders for the image library\n",
    "import os\n",
    "path_to_imgs = path_to_repo + 'data/images/' + query.replace(' ', '_') + '_bs/'  # add bs tag\n",
    "try:\n",
    "    os.makedirs(path_to_imgs)  \n",
    "except OSError as e:\n",
    "    if e.errno != 17:  # if folder already exists, no problem (just overwrite)\n",
    "        raise e  # otherwise, raise e again\n",
    "print \"Creating folder\", path_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35 images!\n"
     ]
    }
   ],
   "source": [
    "for no, url in enumerate(urls):\n",
    "    req = requests.get(url)\n",
    "    with open(path_to_imgs + '%03d' % no + '.jpeg', 'wb') as f:\n",
    "        f.write(req.content)\n",
    "print \"Saved\", len(urls), \"images!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scrape your favorite site and save your valuable new-found data to disk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (**) Note that Bing is a dynamic website that automatically shows more pictures. We only found some small number of images, though. Is it possible to get a hundred or a thousand images? (Honestly, this can be quite a headache, and I don't know if it's easy to do.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
